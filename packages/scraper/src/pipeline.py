"""
çˆ¬è™« Pipeline
ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµæ°´çº¿
"""
import os
import glob
import logging
from typing import List, Optional, Type, Dict
from datetime import datetime, timedelta

import pandas as pd

from .scrapers import BaseScraper, DomainScraper, RealEstateScraper
from .services import DatabaseService, ScoringService, CommuteService
from .models import PropertyData, PropertySource
from .config import settings, TARGET_AREAS

logger = logging.getLogger(__name__)


class ScraperPipeline:
    """
    çˆ¬è™«æ•°æ®å¤„ç†æµæ°´çº¿
    
    å®Œæ•´æµç¨‹:
    1. çˆ¬å–åˆ—è¡¨é¡µ -> è·å–åŸºç¡€æˆ¿æºä¿¡æ¯
    2. çˆ¬å–è¯¦æƒ…é¡µ -> è·å–æè¿°ã€å¯ç”¨æ—¥æœŸç­‰
    3. è¯„åˆ† -> ä½¿ç”¨ AI å¯¹æˆ¿æºè¯„åˆ†
    4. è®¡ç®—é€šå‹¤æ—¶é—´ -> ä½¿ç”¨ Google Maps API
    5. ä¿å­˜åˆ°æ•°æ®åº“
    6. (å¯é€‰) å¯¼å‡º CSV
    """
    
    # æ³¨å†Œçš„çˆ¬è™«ç±»å‹
    SCRAPERS = {
        'domain': DomainScraper,
        'realestate': RealEstateScraper,
    }
    
    def __init__(
        self,
        scraper_types: List[str] = None,
        enable_scoring: bool = True,
        enable_commute: bool = True,
        enable_database: bool = True,
        output_dir: str = None,
        chunk_save_size: int = 100,
        auto_save_list: bool = True,
    ):
        """
        åˆå§‹åŒ– Pipeline
        
        Args:
            scraper_types: è¦ä½¿ç”¨çš„çˆ¬è™«ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤å…¨éƒ¨
            enable_scoring: æ˜¯å¦å¯ç”¨è¯„åˆ†
            enable_commute: æ˜¯å¦å¯ç”¨é€šå‹¤æ—¶é—´è®¡ç®—
            enable_database: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
            output_dir: CSV è¾“å‡ºç›®å½•
        """
        self.scraper_types = scraper_types or list(self.SCRAPERS.keys())
        self.enable_scoring = enable_scoring
        self.enable_commute = enable_commute
        self.enable_database = enable_database
        self.output_dir = output_dir or os.environ.get('OUTPUT_DIR', './output')
        self.chunk_save_size = chunk_save_size
        self.auto_save_list = auto_save_list
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æœåŠ¡
        self.scoring_service = ScoringService() if enable_scoring else None
        self.commute_service = CommuteService() if enable_commute else None
        self.db_service = DatabaseService() if enable_database else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_scraped': 0,
            'total_with_details': 0,
            'total_scored': 0,
            'total_with_commute': 0,
            'total_saved': 0
        }

        # list export stats
        self.stats['list_parts_saved'] = 0
        self.stats['copied_from_history'] = 0
        
        # å†å²æ•°æ®ç¼“å­˜ (house_id -> PropertyData-like dict)
        self._history_cache: Dict[str, dict] = {}
    
    def _load_history_csv(self, university: str) -> Dict[str, dict]:
        """
        åŠ è½½å†å² CSV æ•°æ®ï¼ˆæœ€è¿‘7å¤©å†…çš„å®Œæ•´æ•°æ®æ–‡ä»¶ï¼‰
        è¿”å› house_id -> {description_en, keywords, average_score, ...} çš„æ˜ å°„
        ç”¨äºå¤ç”¨å·²æœ‰çš„è¯¦æƒ…/è¯„åˆ†/é€šå‹¤æ•°æ®ï¼Œé¿å…é‡å¤çˆ¬å–
        """
        cache = {}
        
        # æŸ¥æ‰¾æœ€è¿‘çš„å®Œæ•´ CSV æ–‡ä»¶ï¼ˆä¸æ˜¯ list åˆ†æ®µæ–‡ä»¶ï¼‰
        pattern = os.path.join(self.output_dir, f"{university}_rentdata_*.csv")
        csv_files = glob.glob(pattern)
        
        # è¿‡æ»¤æ‰ list åˆ†æ®µæ–‡ä»¶
        csv_files = [f for f in csv_files if '_list_' not in f]
        
        if not csv_files:
            logger.info(f"æœªæ‰¾åˆ° {university} çš„å†å² CSV æ–‡ä»¶")
            return cache
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        csv_files.sort(key=os.path.getmtime, reverse=True)
        latest_file = csv_files[0]
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨7å¤©å†…
        file_mtime = datetime.fromtimestamp(os.path.getmtime(latest_file))
        if datetime.now() - file_mtime > timedelta(days=7):
            logger.info(f"å†å² CSV æ–‡ä»¶è¶…è¿‡7å¤©ï¼Œä¸ä½¿ç”¨: {latest_file}")
            return cache
        
        logger.info(f"åŠ è½½å†å²æ•°æ®: {latest_file}")
        
        try:
            df = pd.read_csv(latest_file)
            loaded_count = 0
            
            for _, row in df.iterrows():
                house_id = str(row.get('houseId', ''))
                if not house_id:
                    continue
                
                # åªç¼“å­˜æœ‰è¯¦æƒ…æ•°æ®çš„è®°å½•
                desc = row.get('description_en')
                if pd.isna(desc) or not str(desc).strip():
                    continue
                
                cache[house_id] = {
                    'description_en': str(desc) if pd.notna(desc) else None,
                    'description_cn': str(row.get('description_cn', '')) if pd.notna(row.get('description_cn')) else None,
                    'keywords': str(row.get('keywords', '')) if pd.notna(row.get('keywords')) else None,
                    'average_score': float(row.get('average_score', 0)) if pd.notna(row.get('average_score')) else None,
                    'available_date': str(row.get('available_date', '')) if pd.notna(row.get('available_date')) else None,
                    'thumbnail_url': str(row.get('thumbnail_url', '')) if pd.notna(row.get('thumbnail_url')) else None,
                    'commute_times': {}  # åˆå§‹åŒ–é€šå‹¤æ—¶é—´å­—å…¸
                }
                
                # åŠ è½½é€šå‹¤æ—¶é—´åˆ° commute_times å­—å…¸
                for col in df.columns:
                    if col.startswith('commuteTime_'):
                        uni = col.replace('commuteTime_', '')
                        val = row.get(col)
                        if pd.notna(val) and val != '' and val != 0:
                            try:
                                cache[house_id]['commute_times'][uni] = int(val)
                            except (ValueError, TypeError):
                                pass
                
                loaded_count += 1
            
            logger.info(f"ä»å†å² CSV åŠ è½½äº† {loaded_count} æ¡æœ‰è¯¦æƒ…çš„è®°å½•")
            
        except Exception as e:
            logger.error(f"åŠ è½½å†å² CSV å¤±è´¥: {e}")
        
        return cache
    
    def _apply_history_data(self, properties: List[PropertyData], university: str) -> dict:
        """
        å°†å†å²æ•°æ®åº”ç”¨åˆ°å½“å‰æˆ¿æºåˆ—è¡¨
        å¤ç”¨è¯¦æƒ…ã€è¯„åˆ†ã€é€šå‹¤æ—¶é—´
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {'details': N, 'scores': N, 'commute': N}
        """
        stats = {'details': 0, 'scores': 0, 'commute': 0}
        
        if not self._history_cache:
            self._history_cache = self._load_history_csv(university)
        
        if not self._history_cache:
            return stats
        
        for prop in properties:
            house_id_str = str(prop.house_id)
            if house_id_str in self._history_cache:
                hist = self._history_cache[house_id_str]
                
                # å¤ç”¨è¯¦æƒ…æ•°æ®
                if not prop.description_en and hist.get('description_en'):
                    prop.description_en = hist['description_en']
                    prop.description_cn = hist.get('description_cn')
                    prop.keywords = hist.get('keywords')
                    prop.available_date = hist.get('available_date')
                    if hist.get('thumbnail_url'):
                        prop.thumbnail_url = hist['thumbnail_url']
                    stats['details'] += 1
                    logger.debug(f"å¤ç”¨å†å²è¯¦æƒ…: {prop.house_id}")
                
                # å¤ç”¨è¯„åˆ†ï¼ˆå³ä½¿å·²æœ‰è¯¦æƒ…ï¼Œè¯„åˆ†ä¹Ÿå¯èƒ½éœ€è¦å¤ç”¨ï¼‰
                if (not prop.average_score or prop.average_score == 0) and hist.get('average_score'):
                    prop.average_score = hist['average_score']
                    stats['scores'] += 1
                    logger.debug(f"å¤ç”¨å†å²è¯„åˆ†: {prop.house_id} = {prop.average_score}")
                
                # å¤ç”¨å…³é”®è¯
                if not prop.keywords and hist.get('keywords'):
                    prop.keywords = hist['keywords']
                
                # å¤ç”¨é€šå‹¤æ—¶é—´ï¼ˆä» commute_times å­—å…¸ï¼‰
                hist_commute = hist.get('commute_times', {})
                for uni, commute_time in hist_commute.items():
                    if uni not in prop.commute_times or not prop.commute_times.get(uni):
                        prop.commute_times[uni] = commute_time
                        stats['commute'] += 1
                        logger.debug(f"å¤ç”¨å†å²é€šå‹¤: {prop.house_id} -> {uni} = {commute_time}min")
        
        if any(stats.values()):
            logger.info(f"ğŸ“¦ å†å²æ•°æ®å¤ç”¨ç»Ÿè®¡:")
            logger.info(f"   è¯¦æƒ…: {stats['details']} æ¡")
            logger.info(f"   è¯„åˆ†: {stats['scores']} æ¡")
            logger.info(f"   é€šå‹¤: {stats['commute']} æ¡")
        
        return stats
    
    def get_scraper(self, scraper_type: str) -> Optional[BaseScraper]:
        """è·å–çˆ¬è™«å®ä¾‹"""
        scraper_class = self.SCRAPERS.get(scraper_type)
        if scraper_class:
            return scraper_class()
        logger.warning(f"æœªçŸ¥çˆ¬è™«ç±»å‹: {scraper_type}")
        return None
    
    def run(
        self,
        university: str,
        scrape_details: bool = True,
        skip_existing: bool = True
    ) -> List[PropertyData]:
        """
        è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµæ°´çº¿
        
        Args:
            university: å¤§å­¦ä»£ç  (UNSW, USYD, UTS)
            scrape_details: æ˜¯å¦çˆ¬å–è¯¦æƒ…é¡µ
            skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰æ•°æ®
            
        Returns:
            å¤„ç†åçš„æˆ¿äº§æ•°æ®åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹ Pipeline: {university}")
        logger.info(f"çˆ¬è™«ç±»å‹: {self.scraper_types}")
        logger.info("=" * 60)
        
        all_properties = []
        
        # Step 1: çˆ¬å–å„å¹³å°æ•°æ®
        for scraper_type in self.scraper_types:
            logger.info(f"\n{'='*60}")
            logger.info(f"Step 1: ä½¿ç”¨ {scraper_type.upper()} çˆ¬è™«çˆ¬å–æ•°æ®")
            logger.info(f"{'='*60}")
            
            scraper = self.get_scraper(scraper_type)
            if not scraper:
                continue
            
            properties = scraper.scrape_by_university(university)
            logger.info(f"{scraper_type.upper()} çˆ¬å–å®Œæˆ: {len(properties)} ä¸ªæˆ¿æº")

            # å¯¼å‡ºåˆ—è¡¨çº§ CSVï¼ˆä»…åˆ—è¡¨ä¿¡æ¯ï¼‰ï¼ŒæŒ‰ chunk ä¿å­˜ä»¥ä¾¿ç¡®è®¤è¿›åº¦
            if self.auto_save_list and properties:
                parts = self._save_list_chunks(properties, university, scraper_type)
                logger.info(f"å·²ä¿å­˜åˆ—è¡¨ CSV åˆ†æ®µ: {parts} ä¸ª (each {self.chunk_save_size})")
                self.stats['list_parts_saved'] += parts
            
            # Step 1.5: ä¿å­˜åˆå¹¶çš„åˆ—è¡¨ CSV å¹¶ä¸å†å²å¯¹æ¯”
            logger.info(f"\n{'='*60}")
            logger.info(f"Step 1.5: ä¿å­˜å®Œæ•´åˆ—è¡¨å¹¶å¯¹æ¯”å†å²æ•°æ®")
            logger.info(f"{'='*60}")
            
            # ä¿å­˜åˆå¹¶çš„åˆ—è¡¨ CSV
            merged_list_file = self._save_merged_list_csv(properties, university, scraper_type)
            logger.info(f"å®Œæ•´åˆ—è¡¨å·²ä¿å­˜: {merged_list_file}")
            
            # ä»å†å² CSV å¤ç”¨å·²æœ‰çš„è¯¦æƒ…/è¯„åˆ†/é€šå‹¤æ•°æ®
            reuse_stats = self._apply_history_data(properties, university)
            self.stats['copied_from_history'] += reuse_stats['details']
            self.stats['copied_scores'] = self.stats.get('copied_scores', 0) + reuse_stats['scores']
            self.stats['copied_commute'] = self.stats.get('copied_commute', 0) + reuse_stats['commute']

            # ç»Ÿè®¡éœ€è¦å¤„ç†çš„æ•°é‡
            need_details = sum(1 for p in properties if not p.description_en)
            have_details = sum(1 for p in properties if p.description_en)
            need_scores = sum(1 for p in properties if p.description_en and (not p.average_score or p.average_score == 0))
            need_commute = sum(1 for p in properties if university not in p.commute_times)
            
            logger.info(f"\nğŸ“Š åˆ—è¡¨ä¸å†å²å¯¹æ¯”ç»“æœ:")
            logger.info(f"   æ€»æˆ¿æºæ•°: {len(properties)}")
            logger.info(f"   å·²æœ‰è¯¦æƒ…: {have_details} (å¤ç”¨: {reuse_stats['details']})")
            logger.info(f"   éœ€è¦çˆ¬å–è¯¦æƒ…: {need_details}")
            logger.info(f"   éœ€è¦è¯„åˆ†: {need_scores} (å·²å¤ç”¨: {reuse_stats['scores']})")
            logger.info(f"   éœ€è¦é€šå‹¤è®¡ç®—: {need_commute} (å·²å¤ç”¨: {reuse_stats['commute']})")
            
            # Step 2: çˆ¬å–è¯¦æƒ…é¡µ
            if scrape_details and properties:
                logger.info(f"\n{'='*60}")
                logger.info(f"Step 2: çˆ¬å–è¯¦æƒ…é¡µ (ä»…çˆ¬å– {need_details} ä¸ªæ–°æˆ¿æº)")
                logger.info(f"{'='*60}")
                
                properties = scraper.scrape_property_details(
                    properties, 
                    skip_existing=skip_existing
                )
                self.stats['total_with_details'] += sum(
                    1 for p in properties if p.description_en
                )
            
            all_properties.extend(properties)
        
        self.stats['total_scraped'] = len(all_properties)
        logger.info(f"\næ€»å…±çˆ¬å–: {len(all_properties)} ä¸ªæˆ¿æº")
        
        if not all_properties:
            logger.warning("æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®")
            return []
        
        # Step 3: è¯„åˆ†
        if self.enable_scoring and self.scoring_service:
            logger.info(f"\n{'='*60}")
            logger.info("Step 3: æˆ¿äº§è¯„åˆ†")
            logger.info(f"{'='*60}")
            
            all_properties = self.scoring_service.process_properties(
                all_properties,
                skip_existing=skip_existing
            )
            self.stats['total_scored'] = sum(
                1 for p in all_properties if p.average_score
            )
        
        # Step 4: è®¡ç®—é€šå‹¤æ—¶é—´
        if self.enable_commute and self.commute_service:
            logger.info(f"\n{'='*60}")
            logger.info("Step 4: è®¡ç®—é€šå‹¤æ—¶é—´")
            logger.info(f"{'='*60}")
            
            all_properties = self.commute_service.process_properties(
                all_properties,
                university=university,
                skip_existing=skip_existing
            )
            self.stats['total_with_commute'] = sum(
                1 for p in all_properties if p.commute_times.get(university)
            )
        
        # Step 5: ä¿å­˜åˆ°æ•°æ®åº“
        if self.enable_database and self.db_service:
            logger.info(f"\n{'='*60}")
            logger.info("Step 5: ä¿å­˜åˆ°æ•°æ®åº“")
            logger.info(f"{'='*60}")
            
            with self.db_service.session():
                save_stats = self.db_service.save_properties(
                    all_properties, 
                    university
                )
                self.stats['total_saved'] = (
                    save_stats['inserted'] + save_stats['updated']
                )
        
        # Step 6: å¯¼å‡º CSV
        logger.info(f"\n{'='*60}")
        logger.info("Step 6: å¯¼å‡º CSV")
        logger.info(f"{'='*60}")
        
        csv_file = self.export_to_csv(all_properties, university)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_stats(university, csv_file)
        
        return all_properties
    
    def export_to_csv(
        self, 
        properties: List[PropertyData], 
        university: str
    ) -> str:
        """
        å¯¼å‡ºæ•°æ®åˆ° CSV æ–‡ä»¶
        
        Args:
            properties: æˆ¿äº§æ•°æ®åˆ—è¡¨
            university: å¤§å­¦ä»£ç 
            
        Returns:
            CSV æ–‡ä»¶è·¯å¾„
        """
        if not properties:
            return ""
        
        current_date = datetime.now().strftime('%y%m%d')
        filename = f"{university}_rentdata_{current_date}.csv"
        filepath = os.path.join(self.output_dir, filename)
        
        # è½¬æ¢ä¸º DataFrame
        data = []
        for prop in properties:
            row = {
                'pricePerWeek': prop.price_per_week,
                'addressLine1': prop.address_line1,
                'addressLine2': prop.address_line2,
                'bedroomCount': prop.bedroom_count,
                'bathroomCount': prop.bathroom_count,
                'parkingCount': prop.parking_count,
                'propertyType': prop.property_type,
                'houseId': prop.house_id,
                'url': prop.url,
                'description_en': prop.description_en,
                'description_cn': prop.description_cn,
                'keywords': prop.keywords,
                'average_score': prop.average_score,
                'available_date': prop.available_date,
                'published_at': prop.published_at,
                'thumbnail_url': prop.thumbnail_url,
                'source': prop.source.value,
            }
            
            # æ·»åŠ é€šå‹¤æ—¶é—´
            for uni, commute_time in prop.commute_times.items():
                row[f'commuteTime_{uni}'] = commute_time
            
            data.append(row)
        
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
        
        return filepath

    def _save_list_chunks(self, properties: List[PropertyData], university: str, scraper_type: str) -> int:
        """
        å°†åˆ—è¡¨çˆ¬å–ç»“æœæŒ‰ chunk ä¿å­˜ä¸º CSVï¼ˆä¸åŒ…å«è¯„åˆ†/é€šå‹¤ä¿¡æ¯ï¼‰ï¼Œä¾¿äºæŸ¥çœ‹è¿›åº¦
        è¿”å›ä¿å­˜çš„åˆ†æ®µæ•°é‡
        """
        if not properties:
            return 0

        os.makedirs(self.output_dir, exist_ok=True)
        total = len(properties)
        chunk = self.chunk_save_size or total
        parts = 0
        for i in range(0, total, chunk):
            part_props = properties[i:i+chunk]
            current_date = datetime.now().strftime('%y%m%d')
            filename = f"{university}_rentdata_list_{scraper_type}_{current_date}_part{parts+1}.csv"
            filepath = os.path.join(self.output_dir, filename)

            # reuse export logic but avoid adding commute/scores that aren't present yet
            data = []
            for prop in part_props:
                row = {
                    'pricePerWeek': prop.price_per_week,
                    'addressLine1': prop.address_line1,
                    'addressLine2': prop.address_line2,
                    'bedroomCount': prop.bedroom_count,
                    'bathroomCount': prop.bathroom_count,
                    'parkingCount': prop.parking_count,
                    'propertyType': prop.property_type,
                    'houseId': prop.house_id,
                    'url': prop.url,
                    'thumbnail_url': prop.thumbnail_url,
                    'source': prop.source.value,
                }
                data.append(row)

            df = pd.DataFrame(data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            logger.info(f"åˆ—è¡¨åˆ†æ®µå·²å¯¼å‡º: {filepath} (items: {len(part_props)})")
            parts += 1

        return parts
    
    def _save_merged_list_csv(self, properties: List[PropertyData], university: str, scraper_type: str) -> str:
        """
        ä¿å­˜åˆå¹¶çš„å®Œæ•´åˆ—è¡¨ CSVï¼ˆæ‰€æœ‰åŒºåŸŸçš„æˆ¿æºæ±‡æ€»ï¼‰
        ç”¨äºä¸å†å² CSV å¯¹æ¯”
        
        Args:
            properties: æˆ¿äº§åˆ—è¡¨
            university: å¤§å­¦ä»£ç 
            scraper_type: çˆ¬è™«ç±»å‹
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not properties:
            return ""
        
        os.makedirs(self.output_dir, exist_ok=True)
        current_date = datetime.now().strftime('%y%m%d')
        current_time = datetime.now().strftime('%H%M')
        filename = f"{university}_list_merged_{scraper_type}_{current_date}_{current_time}.csv"
        filepath = os.path.join(self.output_dir, filename)
        
        data = []
        for prop in properties:
            row = {
                'houseId': prop.house_id,
                'pricePerWeek': prop.price_per_week,
                'addressLine1': prop.address_line1,
                'addressLine2': prop.address_line2,
                'bedroomCount': prop.bedroom_count,
                'bathroomCount': prop.bathroom_count,
                'parkingCount': prop.parking_count,
                'propertyType': prop.property_type,
                'url': prop.url,
                'thumbnail_url': prop.thumbnail_url,
                'source': prop.source.value,
                'has_history_detail': 'Yes' if prop.description_en else 'No',
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        # ç»Ÿè®¡å¯¹æ¯”ç»“æœ
        has_detail = sum(1 for p in properties if p.description_en)
        no_detail = len(properties) - has_detail
        
        logger.info(f"ğŸ“‹ åˆå¹¶åˆ—è¡¨å·²ä¿å­˜: {filepath}")
        logger.info(f"   - æ€»æ•°: {len(properties)}")
        logger.info(f"   - å·²æœ‰è¯¦æƒ…(å¯å¤ç”¨): {has_detail}")
        logger.info(f"   - éœ€è¦çˆ¬å–è¯¦æƒ…: {no_detail}")
        
        return filepath
    
    def load_from_csv(self, filepath: str) -> List[PropertyData]:
        """
        ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®
        
        Args:
            filepath: CSV æ–‡ä»¶è·¯å¾„
            
        Returns:
            æˆ¿äº§æ•°æ®åˆ—è¡¨
        """
        if not os.path.exists(filepath):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return []
        
        df = pd.read_csv(filepath)
        properties = []
        
        for _, row in df.iterrows():
            # ç¡®å®šæ•°æ®æ¥æº
            source_str = row.get('source', 'domain')
            try:
                source = PropertySource(source_str)
            except:
                source = PropertySource.DOMAIN
            
            prop = PropertyData(
                house_id=str(row.get('houseId', '')),
                source=source,
                price_per_week=int(row.get('pricePerWeek', 0)),
                address_line1=str(row.get('addressLine1', '')),
                address_line2=str(row.get('addressLine2', '')),
                bedroom_count=int(row.get('bedroomCount', 0)),
                bathroom_count=int(row.get('bathroomCount', 0)),
                parking_count=int(row.get('parkingCount', 0)),
                property_type=int(row.get('propertyType', 1)),
                url=str(row.get('url', '')),
                description_en=str(row.get('description_en', '')) if pd.notna(row.get('description_en')) else None,
                description_cn=str(row.get('description_cn', '')) if pd.notna(row.get('description_cn')) else None,
                keywords=str(row.get('keywords', '')) if pd.notna(row.get('keywords')) else None,
                average_score=float(row.get('average_score', 0)) if pd.notna(row.get('average_score')) else None,
                thumbnail_url=str(row.get('thumbnail_url', '')) if pd.notna(row.get('thumbnail_url')) else None,
            )
            
            # åŠ è½½é€šå‹¤æ—¶é—´
            for col in df.columns:
                if col.startswith('commuteTime_'):
                    uni = col.replace('commuteTime_', '')
                    value = row.get(col)
                    if pd.notna(value):
                        prop.commute_times[uni] = int(value)
            
            properties.append(prop)
        
        logger.info(f"ä» CSV åŠ è½½ {len(properties)} ä¸ªæˆ¿æº")
        return properties
    
    def _print_stats(self, university: str, csv_file: str):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print(f"Pipeline å®Œæˆ: {university}")
        print("=" * 60)
        print(f"æ€»çˆ¬å–æ•°é‡: {self.stats['total_scraped']}")
        print(f"ğŸ“¦ å†å²å¤ç”¨ç»Ÿè®¡:")
        print(f"   è¯¦æƒ…å¤ç”¨: {self.stats.get('copied_from_history', 0)}")
        print(f"   è¯„åˆ†å¤ç”¨: {self.stats.get('copied_scores', 0)}")
        print(f"   é€šå‹¤å¤ç”¨: {self.stats.get('copied_commute', 0)}")
        print(f"æœ‰è¯¦æƒ…æè¿°: {self.stats['total_with_details']}")
        print(f"å·²è¯„åˆ†æ•°é‡: {self.stats['total_scored']}")
        print(f"æœ‰é€šå‹¤æ—¶é—´: {self.stats['total_with_commute']}")
        print(f"å·²ä¿å­˜æ•°é‡: {self.stats['total_saved']}")
        print(f"åˆ—è¡¨åˆ†æ®µæ•°: {self.stats.get('list_parts_saved', 0)}")
        print(f"CSV æ–‡ä»¶: {csv_file}")
        print("=" * 60 + "\n")


def run_full_pipeline(
    universities: List[str] = None,
    scraper_types: List[str] = None,
    enable_scoring: bool = True,
    enable_commute: bool = True,
    enable_database: bool = True
):
    """
    è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµæ°´çº¿
    
    Args:
        universities: å¤§å­¦åˆ—è¡¨ï¼Œé»˜è®¤ ['UNSW', 'USYD']
        scraper_types: çˆ¬è™«ç±»å‹ï¼Œé»˜è®¤å…¨éƒ¨
        enable_scoring: æ˜¯å¦è¯„åˆ†
        enable_commute: æ˜¯å¦è®¡ç®—é€šå‹¤æ—¶é—´
        enable_database: æ˜¯å¦ä¿å­˜æ•°æ®åº“
    """
    if universities is None:
        universities = ['UNSW', 'USYD']
    
    pipeline = ScraperPipeline(
        scraper_types=scraper_types,
        enable_scoring=enable_scoring,
        enable_commute=enable_commute,
        enable_database=enable_database
    )
    
    for university in universities:
        try:
            pipeline.run(university)
        except Exception as e:
            logger.error(f"å¤„ç† {university} å¤±è´¥: {e}")
    
    # å¤„ç† UTS (å¤åˆ¶ USYD æ•°æ®å¹¶è®¡ç®—é€šå‹¤æ—¶é—´)
    if 'USYD' in universities and 'UTS' not in universities:
        logger.info("\n" + "=" * 60)
        logger.info("å¤„ç† UTS (åŸºäº USYD æ•°æ®)")
        logger.info("=" * 60)
        
        current_date = datetime.now().strftime('%y%m%d')
        usyd_file = os.path.join(pipeline.output_dir, f"USYD_rentdata_{current_date}.csv")
        uts_file = os.path.join(pipeline.output_dir, f"UTS_rentdata_{current_date}.csv")
        
        if os.path.exists(usyd_file):
            logger.info(f"ä» USYD æ•°æ®å¤åˆ¶: {usyd_file}")
            
            # åŠ è½½ USYD æ•°æ®
            properties = pipeline.load_from_csv(usyd_file)
            logger.info(f"åŠ è½½äº† {len(properties)} ä¸ªæˆ¿æº")
            
            # ä»å†å² UTS CSV å¤ç”¨å·²æœ‰çš„é€šå‹¤æ—¶é—´
            logger.info("\nä»å†å² UTS æ•°æ®å¤ç”¨é€šå‹¤æ—¶é—´...")
            history_cache = pipeline._load_history_csv('UTS')
            
            reused_count = 0
            for prop in properties:
                if prop.house_id and str(prop.house_id) in history_cache:
                    hist = history_cache[str(prop.house_id)]
                    # å¤ç”¨ UTS é€šå‹¤æ—¶é—´
                    if 'UTS' in hist.get('commute_times', {}):
                        prop.commute_times['UTS'] = hist['commute_times']['UTS']
                        reused_count += 1
            
            logger.info(f"å¤ç”¨äº† {reused_count} ä¸ªæˆ¿æºçš„ UTS é€šå‹¤æ—¶é—´")
            
            # ç»Ÿè®¡éœ€è¦è®¡ç®—é€šå‹¤æ—¶é—´çš„æˆ¿æº
            need_commute = sum(1 for p in properties if 'UTS' not in p.commute_times)
            have_commute = len(properties) - need_commute
            
            logger.info(f"\nğŸ“Š UTS é€šå‹¤æ—¶é—´çŠ¶æ€:")
            logger.info(f"   æ€»æˆ¿æºæ•°: {len(properties)}")
            logger.info(f"   å·²æœ‰é€šå‹¤æ—¶é—´: {have_commute} (å¤ç”¨: {reused_count})")
            logger.info(f"   éœ€è¦è®¡ç®—: {need_commute}")
            
            # åªè®¡ç®—ç¼ºå°‘ UTS é€šå‹¤æ—¶é—´çš„æˆ¿æº
            if need_commute > 0 and pipeline.enable_commute and pipeline.commute_service:
                logger.info(f"\nè®¡ç®— {need_commute} ä¸ªæˆ¿æºçš„ UTS é€šå‹¤æ—¶é—´...")
                properties = pipeline.commute_service.process_properties(
                    properties,
                    university='UTS',
                    skip_existing=True  # è·³è¿‡å·²æœ‰é€šå‹¤æ—¶é—´çš„
                )
                
                # ç»Ÿè®¡è®¡ç®—åçš„ç»“æœ
                final_with_commute = sum(1 for p in properties if 'UTS' in p.commute_times)
                logger.info(f"è®¡ç®—å®Œæˆï¼Œç°åœ¨æœ‰ {final_with_commute} ä¸ªæˆ¿æºæœ‰ UTS é€šå‹¤æ—¶é—´")
            else:
                logger.info("æ‰€æœ‰æˆ¿æºéƒ½å·²æœ‰ UTS é€šå‹¤æ—¶é—´ï¼Œè·³è¿‡è®¡ç®—")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if pipeline.enable_database and pipeline.db_service:
                logger.info("\nä¿å­˜åˆ°æ•°æ®åº“...")
                with pipeline.db_service.session():
                    pipeline.db_service.save_properties(properties, 'UTS')
            
            # å¯¼å‡º CSV
            logger.info(f"\nå¯¼å‡º UTS æ•°æ®åˆ°: {uts_file}")
            pipeline.export_to_csv(properties, 'UTS')
            logger.info(f"âœ… UTS æ•°æ®å¤„ç†å®Œæˆ")
        else:
            logger.warning(f"æœªæ‰¾åˆ° USYD æ•°æ®æ–‡ä»¶: {usyd_file}")
            logger.warning("æ— æ³•ç”Ÿæˆ UTS æ•°æ®")


