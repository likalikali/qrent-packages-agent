#!/usr/bin/env python3
"""
å•ç‹¬å¤„ç† USYD æ•°æ®
- ä½¿ç”¨å·²æœ‰çš„åˆ—è¡¨ CSVï¼ˆä¸é‡æ–°çˆ¬å–åˆ—è¡¨é¡µï¼‰
- ä»å†å²æ•°æ®å¤ç”¨è¯¦æƒ…/è¯„åˆ†/é€šå‹¤
- åªçˆ¬å–ç¼ºå¤±çš„è¯¦æƒ…é¡µ
- åªè¯„åˆ†ç¼ºå¤±çš„æˆ¿æº
- åªè®¡ç®—ç¼ºå¤±çš„é€šå‹¤æ—¶é—´
"""
import os
import sys
import logging
import pandas as pd
from datetime import datetime

# è®¾ç½®è·¯å¾„
sys.path.insert(0, '.')
os.environ['PYTHONUNBUFFERED'] = '1'

from src.pipeline import ScraperPipeline
from src.services import ScoringService, CommuteService, DatabaseService
from src.models import PropertyData, PropertySource
from src.utils.logger import setup_logger

logger = setup_logger("process_usyd")

def load_list_csv(filepath: str) -> list:
    """ä»åˆ—è¡¨ CSV åŠ è½½æˆ¿æºæ•°æ®"""
    if not os.path.exists(filepath):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return []
    
    df = pd.read_csv(filepath)
    properties = []
    
    for _, row in df.iterrows():
        source_str = row.get('source', 'realestate')
        try:
            source = PropertySource(source_str)
        except:
            source = PropertySource.REALESTATE
        
        prop = PropertyData(
            house_id=str(row.get('houseId', '')),
            source=source,
            price_per_week=int(row.get('pricePerWeek', 0)),
            address_line1=str(row.get('addressLine1', '')),
            address_line2=str(row.get('addressLine2', '')),
            bedroom_count=int(row.get('bedroomCount', 0)),
            bathroom_count=int(row.get('bathroomCount', 0)),
            parking_count=int(row.get('parkingCount', 0)),
            property_type=int(row.get('propertyType', 1)),
            url=str(row.get('url', '')),
            thumbnail_url=str(row.get('thumbnail_url', '')) if pd.notna(row.get('thumbnail_url')) else None,
        )
        properties.append(prop)
    
    logger.info(f"ä»åˆ—è¡¨ CSV åŠ è½½äº† {len(properties)} ä¸ªæˆ¿æº")
    return properties


def main():
    university = 'USYD'
    
    # ä½¿ç”¨ä»Šå¤©çš„åˆå¹¶åˆ—è¡¨æ–‡ä»¶
    list_file = './output/USYD_list_merged_realestate_251222_0418.csv'
    
    print("=" * 60)
    print(f"å•ç‹¬å¤„ç† {university} æ•°æ®")
    print(f"åˆ—è¡¨æ–‡ä»¶: {list_file}")
    print("=" * 60)
    
    # åˆ›å»º Pipelineï¼ˆä¸å¯ç”¨çˆ¬è™«ï¼Œåªç”¨æœåŠ¡ï¼‰
    pipeline = ScraperPipeline(
        scraper_types=[],  # ä¸ä½¿ç”¨çˆ¬è™«
        enable_scoring=True,
        enable_commute=True,
        enable_database=True,
        output_dir='./output'
    )
    
    # Step 1: åŠ è½½åˆ—è¡¨æ•°æ®
    logger.info(f"\n{'='*60}")
    logger.info("Step 1: åŠ è½½å·²æœ‰åˆ—è¡¨æ•°æ®")
    logger.info(f"{'='*60}")
    
    properties = load_list_csv(list_file)
    if not properties:
        logger.error("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
        return
    
    # Step 2: ä»å†å²æ•°æ®å¤ç”¨è¯¦æƒ…/è¯„åˆ†/é€šå‹¤
    logger.info(f"\n{'='*60}")
    logger.info("Step 2: ä»å†å²æ•°æ®å¤ç”¨è¯¦æƒ…/è¯„åˆ†/é€šå‹¤")
    logger.info(f"{'='*60}")
    
    reuse_stats = pipeline._apply_history_data(properties, university)
    
    # ç»Ÿè®¡
    need_details = sum(1 for p in properties if not p.description_en)
    have_details = sum(1 for p in properties if p.description_en)
    need_scores = sum(1 for p in properties if p.description_en and (not p.average_score or p.average_score == 0))
    need_commute = sum(1 for p in properties if university not in p.commute_times)
    
    logger.info(f"\nğŸ“Š æ•°æ®çŠ¶æ€:")
    logger.info(f"   æ€»æˆ¿æºæ•°: {len(properties)}")
    logger.info(f"   å·²æœ‰è¯¦æƒ…: {have_details} (å¤ç”¨: {reuse_stats['details']})")
    logger.info(f"   ç¼ºå°‘è¯¦æƒ…: {need_details}")
    logger.info(f"   éœ€è¦è¯„åˆ†: {need_scores} (å·²å¤ç”¨: {reuse_stats['scores']})")
    logger.info(f"   éœ€è¦é€šå‹¤: {need_commute} (å·²å¤ç”¨: {reuse_stats['commute']})")
    
    # Step 3: çˆ¬å–ç¼ºå°‘è¯¦æƒ…çš„æˆ¿æº
    logger.info(f"\n{'='*60}")
    logger.info(f"Step 3: çˆ¬å–è¯¦æƒ…é¡µ (éœ€è¦çˆ¬å–: {need_details})")
    logger.info(f"{'='*60}")
    
    if need_details > 0:
        # ä½¿ç”¨ pipeline çš„æ–¹å¼è·å–æ­£ç¡®é…ç½®çš„çˆ¬è™«ï¼ˆåŒ…å«åçˆ¬è™«è®¾ç½®ï¼‰
        scraper = pipeline.get_scraper('realestate')
        
        if scraper:
            # å…ˆé‡ç½® profileï¼Œç¡®ä¿å¹²å‡€çš„æµè§ˆå™¨çŠ¶æ€
            logger.info("é‡ç½®æµè§ˆå™¨ profile...")
            scraper._reset_profile()
            
            # åªçˆ¬å–ç¼ºå°‘è¯¦æƒ…çš„æˆ¿æº
            properties = scraper.scrape_property_details(
                properties,
                skip_existing=True  # è·³è¿‡å·²æœ‰è¯¦æƒ…çš„
            )
            
            # æ›´æ–°ç»Ÿè®¡
            have_details_after = sum(1 for p in properties if p.description_en)
            logger.info(f"è¯¦æƒ…çˆ¬å–å: {have_details_after} ä¸ªæœ‰è¯¦æƒ… (æ–°å¢: {have_details_after - have_details})")
        else:
            logger.error("æ— æ³•è·å– RealEstate çˆ¬è™«å®ä¾‹")
    else:
        logger.info("æ‰€æœ‰æˆ¿æºéƒ½å·²æœ‰è¯¦æƒ…ï¼Œè·³è¿‡çˆ¬å–")
    
    # åªä¿ç•™æœ‰è¯¦æƒ…çš„æˆ¿æºè¿›è¡Œåç»­å¤„ç†
    properties_with_details = [p for p in properties if p.description_en]
    logger.info(f"å°†å¤„ç† {len(properties_with_details)} ä¸ªæœ‰è¯¦æƒ…çš„æˆ¿æº")
    
    # Step 4: è¯„åˆ†
    if pipeline.scoring_service:
        logger.info(f"\n{'='*60}")
        logger.info(f"Step 4: æˆ¿äº§è¯„åˆ† (éœ€è¦è¯„åˆ†: {need_scores})")
        logger.info(f"{'='*60}")
        
        properties_with_details = pipeline.scoring_service.process_properties(
            properties_with_details,
            skip_existing=True
        )
    
    # Step 5: è®¡ç®—é€šå‹¤æ—¶é—´
    if pipeline.commute_service:
        logger.info(f"\n{'='*60}")
        logger.info(f"Step 5: è®¡ç®—é€šå‹¤æ—¶é—´ (éœ€è¦è®¡ç®—: {need_commute})")
        logger.info(f"{'='*60}")
        
        properties_with_details = pipeline.commute_service.process_properties(
            properties_with_details,
            university=university,
            skip_existing=True
        )
    
    # Step 6: ä¿å­˜åˆ°æ•°æ®åº“
    if pipeline.db_service:
        logger.info(f"\n{'='*60}")
        logger.info("Step 6: ä¿å­˜åˆ°æ•°æ®åº“")
        logger.info(f"{'='*60}")
        
        try:
            with pipeline.db_service.session():
                save_stats = pipeline.db_service.save_properties(properties_with_details, university)
                logger.info(f"ä¿å­˜å®Œæˆ: æ–°å¢ {save_stats['inserted']}, æ›´æ–° {save_stats['updated']}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
    
    # Step 7: å¯¼å‡º CSV
    logger.info(f"\n{'='*60}")
    logger.info("Step 7: å¯¼å‡º CSV")
    logger.info(f"{'='*60}")
    
    csv_file = pipeline.export_to_csv(properties_with_details, university)
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 60)
    print(f"å¤„ç†å®Œæˆ: {university}")
    print("=" * 60)
    print(f"æ€»æˆ¿æºæ•°: {len(properties)}")
    print(f"æœ‰è¯¦æƒ…çš„: {len(properties_with_details)}")
    print(f"å·²è¯„åˆ†: {sum(1 for p in properties_with_details if p.average_score)}")
    print(f"æœ‰é€šå‹¤æ—¶é—´: {sum(1 for p in properties_with_details if p.commute_times.get(university))}")
    print(f"CSV æ–‡ä»¶: {csv_file}")
    print("=" * 60)


if __name__ == '__main__':
    main()
